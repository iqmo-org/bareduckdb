name: Benchmarks

on:
  push:
    branches: [main]
  pull_request:
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    name: 'Benchmarks'
    runs-on: ubuntu-latest

    permissions:
      contents: read
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@main
        with:
          fetch-depth: 0
          submodules: false

      - name: Initialize DuckDB submodule (headers only)
        shell: bash
        run: |
          git submodule update --init external/duckdb
          cd external/duckdb
          git sparse-checkout init --cone
          git sparse-checkout set src/include
          cd ../..

      - name: Set up Python
        uses: actions/setup-python@main
        with:
          python-version: '3.12'

      - name: Install UV
        uses: astral-sh/setup-uv@v7
        with:
          enable-cache: true
          cache-suffix: -benchmarks
      - name: Check resources
        run: |
          cat /proc/meminfo | head -5
          ulimit -a
          nproc
      - name: Setup Compiler
        shell: bash
        run: |
          sudo apt-get update
          # sudo apt-get install -y gcc-14 g++-14 gdb
          sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-14 100
          sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-14 100
          echo "CC=gcc-14" >> $GITHUB_ENV
          echo "CXX=g++-14" >> $GITHUB_ENV

      - name: Build current repo
        run: uv sync

      # - name: Debug segfault
      #   run: |
      #     uv run gdb -batch \
      #       -ex "set confirm off" \
      #       -ex "run" \
      #       -ex "bt full" \
      #       -ex "info registers" \
      #       --args python -m pytest tests/benchmarks/test_sql_cases.py \
      #         -k "limits_topn_large" \
      #         -o "addopts=" \
      #         --count=20 \
      #         -v \
      #         --no-cov \
      #         2>&1 | tail -500

      - name: Run benchmarks
        run: |
          
          uv venv .venv312 --clear -p cp312
          UV_PROJECT_ENVIRONMENT=.venv312 uv sync --reinstall
          UV_PROJECT_ENVIRONMENT=.venv312 uv run pytest tests/benchmarks \
            --forked \
            --count=3 \
            -n 0 \
            --no-cov \
            -v \
            --benchmark-suffix=dev312 \
            --registration-modes=parquet,arrow
    
      - name: Run benchmarks 314
        run: |
          
          uv venv .venv314 --clear -p cp314
          UV_PROJECT_ENVIRONMENT=.venv314 uv sync --reinstall
          UV_PROJECT_ENVIRONMENT=.venv314 uv run pytest tests/benchmarks \
            --forked \
            --count=3 \
            -n 0 \
            --no-cov \
            -v \
            --benchmark-suffix=dev314 \
            --registration-modes=parquet,arrow

      - name: Run benchmarks 314t
        run: |
          
          uv venv .venv314t --clear -p cp314t
          UV_PROJECT_ENVIRONMENT=.venv314t uv sync --reinstall
          UV_PROJECT_ENVIRONMENT=.venv314t uv run pytest tests/benchmarks \
            --forked \
            --count=3 \
            -n 0 \
            --no-cov \
            -v \
            --benchmark-suffix=dev314t \
            --registration-modes=parquet,arrow
            

      - name: Run benchmarks - duckdb
        run: |
            uv venv --clear .venv-duckdb -p 3.12
            uv pip install duckdb pyarrow pytest pytest-forked pytest-repeat --python .venv-duckdb/bin/python
            .venv-duckdb/bin/pytest tests/benchmarks \
              --confcutdir=tests/benchmarks \
              -o "addopts=" \
              --forked \
              --count=3 \
              -v \
              --use-duckdb --benchmark-suffix=duckdb \
              --registration-modes=parquet,arrow


      - name: Run benchmarks - bareduckdb release
        run: |
          uv venv --clear .venv-bareduckdb-release -p 3.12
          uv pip install bareduckdb pyarrow pytest pytest-forked pytest-repeat --python .venv-bareduckdb-release/bin/python
          .venv-bareduckdb-release/bin/pytest tests/benchmarks \
            -o "addopts=" \
            --forked \
            --count=3 \
            -v \
            --benchmark-suffix=release

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: benchmark-results/

      - name: Generate comparison
        run: |
          uv run python tests/benchmarks/compare_results.py benchmark-results > comparison.md
          cat comparison.md

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          gh pr comment ${{ github.event.pull_request.number }} --body-file comparison.md
